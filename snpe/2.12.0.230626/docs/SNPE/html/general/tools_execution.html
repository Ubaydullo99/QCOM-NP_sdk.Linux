

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>snpe-net-run &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.12.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>snpe-net-run</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="snpe-net-run">
<h1>snpe-net-run<a class="headerlink" href="#snpe-net-run" title="Permalink to this headline">¶</a></h1>
<p>snpe-net-run loads a DLC file, loads the data for the input
tensor(s), and executes the network on the specified runtime.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool that loads and executes a neural network using the SDK API.


REQUIRED ARGUMENTS:
-------------------
--container  &lt;FILE&gt;   Path to the DL container containing the network.
--input_list &lt;FILE&gt;   Path to a file listing the inputs for the network.


OPTIONAL ARGUMENTS:
-------------------
--use_gpu             Use the GPU runtime.
--use_dsp             Use the DSP fixed point runtime.
--debug               Specifies that output from all layers of the network
                    will be saved.
--output_dir=&lt;val&gt;
                    The directory to save output to. Defaults to ./output
--storage_dir=&lt;val&gt;
                    The directory to store metadata files
--encoding_type=&lt;val&gt;
                    Specifies the encoding type of input file. Valid settings are &quot;nv21&quot;.
                    Cannot be combined with --userbuffer*.
--use_native_input_files
                    Specifies to consume the input file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--use_native_output_files
                    Specifies to write the output file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--userbuffer_auto
                    Specifies to use userbuffer for input and output, with auto detection of types enabled.
                    Must be used with user specified buffer. Cannot be combined with --encoding_type.
--userbuffer_float
                    Specifies to use userbuffer for inference, and the input type is float.
                    Cannot be combined with --encoding_type.
--userbuffer_floatN=&lt;val&gt;
                    Specifies to use userbuffer for inference, and the input type is float 16 or float 32.
                    Cannot be combined with --encoding_type.
--userbuffer_tf8      Specifies to use userbuffer for inference, and the input type is tf8exact0.
                    Cannot be combined with --encoding_type.
--userbuffer_tfN=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                    Must be used with user specified buffer.
--userbuffer_float_output
                    Overrides the userbuffer output used for inference, and the output type is float. Must be used with user
                    specified buffer.
--userbuffer_floatN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is float 16 or float 32. Must be used with user
                    specified buffer.
--userbuffer_tfN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                    Must be used with user specified buffer.
--userbuffer_tf8_output
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0.
--userbuffer_uintN_output=&lt;val&gt;
                    Overrides the userbuffer output used for inference, and the output type is Uint N. Must be used with user
                    specified buffer.
--static_min_max  Specifies to use quantization parameters from the model instead of
                    input specific quantization. Used in conjunction with --userbuffer_tf8.
--resizable_dim=&lt;val&gt;
                    Specifies the maximum number that resizable dimensions can grow into.
                    Used as a hint to create UserBuffers for models with dynamic sized outputs. Should be a
                    positive integer and is not applicable when using ITensor.
--userbuffer_glbuffer
                    [EXPERIMENTAL]  Specifies to use userbuffer for inference, and the input source is OpenGL buffer.
                    Cannot be combined with --encoding_type.
                    GL buffer mode is only supported on Android OS.
--data_type_map=&lt;val&gt;
                    Sets data type of IO buffers during prepare.
                    Arguments should be provided in the following format:
                    --data_type_map buffer_name1=buffer_name1_data_type --data_type_map buffer_name2=buffer_name2_data_type
                    Data Type can have the following values: float32, fixedPoint8, fixedPoint16
--tensor_mode=&lt;val&gt;
                    Sets type of tensor to use.
                    Arguments should be provided in the following format:
                    --tensor_mode itensor
                    Data Type can have the following values: userBuffer, itensor
--perf_profile=&lt;val&gt;
                    Specifies perf profile to set. Valid settings are &quot;low_balanced&quot; , &quot;balanced&quot; , &quot;default&quot;,
                    &quot;high_performance&quot; ,&quot;sustained_high_performance&quot;, &quot;burst&quot;, &quot;low_power_saver&quot;, &quot;power_saver&quot;,
                    &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, and &quot;system_settings&quot;.
--profiling_level=&lt;val&gt;
                    Specifies the profiling level.  Valid settings are &quot;off&quot;, &quot;basic&quot;, &quot;moderate&quot;, &quot;detailed&quot;, and &quot;linting&quot;.
                    Default is detailed.
--enable_cpu_fallback
                    Enables cpu fallback functionality. Defaults to disable mode.
--input_name=&lt;val&gt;
                    Specifies the name of input for which dimensions are specified.
--input_dimensions=&lt;val&gt;
                    Specifies new dimensions for input whose name is specified in input_name. e.g. &quot;1,224,224,3&quot;.
                    For multiple inputs, specify --input_name and --input_dimensions multiple times.
--gpu_mode=&lt;val&gt;  Specifies gpu operation mode. Valid settings are &quot;default&quot;, &quot;float16&quot;.
                    default = float32 math and float16 storage (equiv. use_gpu arg).
                    float16 = float16 math and float16 storage.
--enable_init_cache
                    Enable init caching mode to accelerate the network building process. Defaults to disable.
--platform_options=&lt;val&gt;
                    Specifies value to pass as platform options.
--priority_hint=&lt;val&gt;
                    Specifies hint for priority level.  Valid settings are &quot;low&quot;, &quot;normal&quot;, &quot;normal_high&quot;, &quot;high&quot;. Defaults to normal.
                    Note: &quot;normal_high&quot; is only available on DSP.
--inferences_per_duration=&lt;val&gt;
                    Specifies the number of inferences in specific duration (in seconds). e.g. &quot;10,20&quot;.
--runtime_order=&lt;val&gt;
                    Specifies the order of precedence for runtime e.g  cpu_float32, dsp_fixed8_tf etc
                    Valid values are:-
                    cpu_float32 (Snapdragon CPU)       = Data &amp; Math: float 32bit
                    gpu_float32_16_hybrid (Adreno GPU) = Data: float 16bit Math: float 32bit
                    dsp_fixed8_tf (Hexagon DSP)        = Data &amp; Math: 8bit fixed point Tensorflow style format
                    gpu_float16 (Adreno GPU)           = Data: float 16bit Math: float 16bit
--set_output_tensors=&lt;val&gt;
                    Specifies a comma separated list of tensors to be output after execution.
--set_unconsumed_as_output
                    Sets all unconsumed tensors as outputs.
                    aip_fixed8_tf (Snapdragon HTA+HVX) = Data &amp; Math: 8bit fixed point Tensorflow style format
                    cpu (Snapdragon CPU)               = Same as cpu_float32
                    gpu (Adreno GPU)                   = Same as gpu_float32_16_hybrid
                    dsp (Hexagon DSP)                  = Same as dsp_fixed8_tf
                    aip (Snapdragon HTA+HVX)           = Same as aip_fixed8_tf
--udo_package_path=&lt;val&gt;
                    Path to the registration library for UDO package(s).
                    Optionally, user can provide multiple packages as a comma-separated list.
--duration=&lt;val&gt;      Specified the duration of the run in seconds. Loops over the input_list until this amount of time has transpired.
--dbglogs
--timeout=&lt;val&gt;       Execution terminated when exceeding time limit (in microseconds). Only valid for HTP (dsp v68+) runtime.
--userlogs=&lt;val&gt;      Specifies the user level logging as level,&lt;optional logPath&gt;.
                      Valid values are: &quot;warn&quot;, &quot;verbose&quot;, &quot;info&quot;, &quot;error&quot;, &quot;fatal&quot;
--help                Show this help message.
--version             Show SDK Version Number.
</pre></div>
</div>
<div class="line-block">
<div class="line">This binary outputs raw output tensors into the output folder
by default. Examples of using snpe-net-run can be found in
<a class="reference external" href="tutorial_inceptionv3.html">Running the Inception v3 Model</a> tutorial.</div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>Running batched inputs:</em></p>
<blockquote>
<div><ul>
<li><p>snpe-net-run is able to automatically batch the input
data. The batch size is indicated in the model container
(DLC file) but can also be set using the
“input_dimensions” argument passed to snpe-net-run. Users
do not need to batch their input data. If the input data
is not batch, the input size needs to be a multiple of
the size of the input data files. snpe-net-run would
group the provided inputs into batches and pad the
incomplete batches (if present) with zeros.</p>
<p>In the example below, the model is set to accept batches
of three inputs. So, the inputs are automatically grouped
together to form batches by snpe-net-run and padding is
done to the final batch. Note that there are five output
files generated by snpe-net-run:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span> …
Processing DNN input(s):
cropped/notice_sign.raw
cropped/trash_bin.raw
cropped/plastic_cup.raw
Processing DNN input(s):
cropped/handicap_sign.raw
cropped/chairs.raw
Applying padding
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>input_list argument:</em></p>
<blockquote>
<div><ul>
<li><p>snpe-net-run can take multiple input files as input data
per iteration, and specify multiple output names, in an
input list file formated as below:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_name&gt;[&lt;space&gt;&lt;output_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p>The first line starting with a “#” specifies the output
layers’ names. If there is more than one output, a
whitespace should be used as a delimiter. Following the
first line, you can use multiple lines to supply input
files, one line per iteration, and each line only supply
one layer.If there is more than one input per line, a
whitespace should be used as a delimiter.</p>
<p>Here is an example, where the layer names are “Input_1”
and “Input_2”, and inputs are located in the path
“Placeholder_1/real_input_inputs_1/”. Its input list file
should look like this:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#Output_1 Output_2
Input_1:=Placeholder_1/real_input_inputs_1/0-0#e6fb51.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/0-1#8a171b.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/1-0#67c965.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/1-1#54f1ff.rawtensor
Input_1:=Placeholder_1/real_input_inputs_1/2-0#b42dc6.rawtensor Input_2:=Placeholder_1/real_input_inputs_1/2-1#346a0e.rawtensor
</pre></div>
</div>
<p><strong>Note:</strong> If the batch dimension of the model is greater
than 1, the number of batch elements in the input file
has to either match the batch dimension specified in the
DLC or it has to be one. In the latter case, snpe-net-run
will combine multiple lines into a single input tensor.</p>
</li>
</ul>
</div></blockquote>
</li>
<li><p><em>Running AIP Runtime:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>AIP Runtime requires a DLC which was quantized, and HTA
sections were generated offline.</p></li>
<li><p>AIP Runtime does not support debug_mode</p></li>
<li><p>AIP Runtime requires a DLC with all the layers
partitioned to HTA to support batched inputs</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-parallel-run">
<h1>snpe-parallel-run<a class="headerlink" href="#snpe-parallel-run" title="Permalink to this headline">¶</a></h1>
<p>snpe-parallel-run loads a DLC file, loads the data for the
input tensor(s), and executes the network on the specified
runtime. This app is similar to snpe-net-run, but is able to
run multiple threads of inference on the same network for
benchmarking purposes.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool that loads and executes one or more neural networks on different threads with optional asynchronous input/output processing using SDK APIs.


REQUIRED ARGUMENTS:
-------------------
--container  &lt;FILE&gt;   Path to the DL container containing the network.
--input_list &lt;FILE&gt;   Path to a file listing the inputs for the network.
--perf_profile &lt;VAL&gt;
                    Specifies perf profile to set. Valid settings are &quot;balanced&quot; , &quot;default&quot; , &quot;high_performance&quot; , &quot;sustained_high_performance&quot; , &quot;burst&quot; , &quot;power_saver&quot;, &quot;low_power_saver&quot;, &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, &quot;low_balanced&quot;, and &quot;system_settings&quot;.
                    NOTE: &quot;balanced&quot; and &quot;default&quot; are the same.  &quot;default&quot; is being deprecated in the future.
--cpu_fallback        Enables cpu fallback functionality. Valid settings are &quot;false&quot;, &quot;true&quot;.
--runtime_order &lt;VAL,VAL,VAL,..&gt;
                    Specifies the order of precedence for runtime e.g cpu,gpu etc. Valid values are:-
                                cpu_float32 (Snapdragon CPU)       = Data &amp; Math: float 32bit
                                gpu_float32_16_hybrid (Adreno GPU) = Data: float 16bit Math: float 32bit
                                dsp_fixed8_tf (Hexagon DSP)        = Data &amp; Math: 8bit fixed point Tensorflow style format
                                gpu_float16 (Adreno GPU)           = Data: float 16bit Math: float 16bit
                                aip_fixed8_tf (Snapdragon HTA+HVX) = Data &amp; Math: 8bit fixed point Tensorflow style format
                                cpu (Snapdragon CPU)               = Same as cpu_float32
                                gpu (Adreno GPU)                   = Same as gpu_float32_16_hybrid
                                dsp (Hexagon DSP)                  = Same as dsp_fixed8_tf
                                aip (Snapdragon HTA+HVX)           = Same as aip_fixed8_tf
--use_cpu             Use the CPU runtime.
--use_gpu             Use the GPU float32 runtime.
--use_gpu_fp16        Use the GPU float16 runtime.
--use_dsp             Use the DSP fixed point runtime.
--use_aip             Use the AIP fixed point runtime.


OPTIONAL ARGUMENTS:
-------------------
--userbuffer_float    Specifies to use userbuffer for inference, and the input type is float.
--userbuffer_tf8      Specifies to use userbuffer for inference, and the input type is tf8exact0.
--userbuffer_auto     Specifies to use userbuffer with automatic input and output type detection for inference.
--use_native_input_files
                    Specifies to consume the input file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--use_native_output_files
                    Specifies to write the output file(s) in their native data type(s).
                    Must be used with --userbuffer_xxx.
--input_name &lt;INPUT_NAME&gt;
                    Specifies the name of input for which dimensions are specified.
--input_dimensions &lt;INPUT_DIM&gt;
                    Specifies new dimensions for input whose name is specified in input_name. e.g. &quot;1,224,224,3&quot;.
--output_dir &lt;DIR&gt;    The directory to save result files
--static_min_max      Specifies to use quantization parameters from the model instead of
                    input specific quantization. Used in conjunction with --userbuffer_tf8.
--userbuffer_float_output
                    Overrides the userbuffer output used for inference, and the output type is float.
                    Must be used with user specified buffer.
--userbuffer_tf8_output
                    Overrides the userbuffer output used for inference, and the output type is tf8exact0.
                    Must be used with user specified buffer.
--enable_init_cache   Enable init caching mode to accelerate the network building process. Defaults to disable.
--profiling_level     Specifies the profiling level.  Valid settings are &quot;off&quot;, &quot;basic&quot;, &quot;moderate&quot;, &quot;detailed&quot;, and &quot;linting&quot;.
                      Default is off.
--platform_options    Specifies value to pass as platform options.  Valid settings: &quot;HtaDLBC:ON/OFF&quot;, &quot;unsignedPD:ON/OFF&quot;.
--set_output_tensors  Specifies a comma separated list of tensors to be output after execution.
--userlogs &lt;VAL&gt;      Specifies the user level logging as level,&lt;optional logPath&gt;.
--version             Show SDK Version Number.
--help                Show this help message.
</pre></div>
</div>
<div class="line-block">
<div class="line">Additional details:</div>
</div>
<ul>
<li><p><em>Required runtime argument:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>For the required arguments pertaining to runtime
specification, either –runtime_order OR –use_cpu OR –use_gpu etc.
needs to be specified. The following example demonstrates
an equivalent command using either of these options.</p></li>
</ul>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --use_dsp --use_gpu --userbuffer_auto
</pre></div>
</div>
<p>is equivalent to</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --runtime_order dsp,gpu --userbuffer_auto
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><em>Spawning multiple threads:</em></p>
<blockquote>
<div><ul class="simple">
<li><p>snpe-parallel-run is able to create multiple threads to
execute identical inference passes.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>In the example below, the given command has the required
arguments for container and input list given. After these
2 options, the remaining options form a repeating
sequence that corresponds to each thread. In this
example, we have varied the runtimes specified for each
thread (one for dsp, another for gpu, and the last one
for dsp).</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>snpe-parallel-run --container container.dlc --input_list input_list.txt
--perf_profile burst --cpu_fallback true --use_dsp --userbuffer_auto
--perf_profile burst --cpu_fallback true --use_gpu --userbuffer_auto
--perf_profile burst --cpu_fallback true --use_dsp --userbuffer_auto
</pre></div>
</div>
<p>When this command is executed, the following section of
output is observed:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>...
Processing DNN input(s):
input.raw
PSNPE start executing...
runtimes: dsp_fixed8_tf gpu_float32_16_hybrid dsp_fixed8_tf - Mode :0- Number of images processed: x
    Build time: x seconds.
...
</pre></div>
</div>
<p>Note that the number of runtimes listed corresponds to
the number of threads specified, as well as the order in
which those threads were specified.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-throughput-net-run">
<h1>snpe-throughput-net-run<a class="headerlink" href="#snpe-throughput-net-run" title="Permalink to this headline">¶</a></h1>
<p>snpe-throughput-net-run concurrently runs multiple instances of
SNPE for a certain duration of time and measures inference
throughput. Each instance of SNPE can have its own model,
designated runtime and performance profile. Please note that
the <cite>–duration</cite> parameter is common for all instances of SNPE
created.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
Tool to load and execute concurrent SNPE objects using the SDK API.


REQUIRED ARGUMENTS:
-------------------
    --container  &lt;FILE&gt;   Path to the DL container containing the network.
    --duration   &lt;VAL&gt;    Duration of time (in seconds) to run network execution.
    --use_cpu             Use the CPU runtime.
    --use_gpu             Use the GPU float32 runtime.
    --use_gpu_fp16        Use the GPU float16 runtime.
    --use_dsp             Use the DSP fixed point runtime.
    --perf_profile &lt;VAL&gt;  Specifies perf profile to set. Valid settings are &quot;balanced&quot; , &quot;default&quot; , &quot;high_performance&quot; ,
                        &quot;sustained_high_performance&quot; , &quot;burst&quot; , &quot;power_saver&quot;, &quot;low_power_saver&quot;, &quot;high_power_saver&quot;, &quot;extreme_power_saver&quot;, &quot;low_balanced&quot;, and &quot;system_settings&quot;.
                        NOTE: &quot;balanced&quot; and &quot;default&quot; are the same.  &quot;default&quot; is being deprecated in the future.
    --use_aip             Use the AIP fixed point runtime.


OPTIONAL ARGUMENTS:
-------------------
    --debug                               Specifies that output from all layers of the network
                                        will be saved.
    --userbuffer_auto                     Specifies to use userbuffer for input and output, with auto detection of types enabled.
                                        Must be used with user specified buffer.
    --userbuffer_float                    Specifies to use userbuffer for inference, and the input type is float.
                                        Must be used with user specified buffer.
    --userbuffer_floatN                   Specifies to use userbuffer for inference, and the input type is float16 or float32.
                                        Must be used with user specified buffer.
    --userbuffer_tf8                      Specifies to use userbuffer for inference, and the input type is tf8exact0.
                                        Must be used with user specified buffer.
    --userbuffer_tfN                      Specifies to use userbuffer for inference, and the input type is tf8exact0 or tf16exact0.
                                        Must be used with user specified buffer.
    --userbuffer_float_output             Overrides the userbuffer output used for inference, and the output type is float.
                                        Must be used with user specified buffer.
    --userbuffer_floatN_output            Overrides the userbuffer output used for inference, and the output type is float16 or float32.
                                        Must be used with user specified buffer.
    --userbuffer_tf8_output               Overrides the userbuffer output used for inference, and the output type is tf8exact0.
                                        Must be used with user specified buffer.
    --userbuffer_tfN_output               Overrides the userbuffer output used for inference, and the output type is tf8exact0 or tf16exact0.
                                        Must be used with user specified buffer.
    --storage_dir &lt;DIR&gt;                   The directory to store metadata files
    --version                             Show SDK Version Number.
    --iterations &lt;VAL&gt;                    Number of times to iterate through entire input list
    --verbose                             Print more debug information.
    --skip_execute                        Don&#39;t do execution (just graph build/teardown)
    --json  &lt;FILE&gt;                        Generated JSON report.
    --input_raw &lt;FILE&gt;                    Path to raw inputs for the network, seperated by &quot;,&quot;.
    --enable_cpu_fallback                 Enables cpu fallback functionality. Defaults to disable mode.
    --udo_package_path &lt;VAL,VAL&gt;
                                        Path to UDO package with registration library for UDOs.
                                        Optionally, user can provide multiple packages as a comma-separated list.
    --priority_hint &lt;VAL&gt;
                                        Specifies hint for priority level. Valid settings are &quot;low&quot;, &quot;normal&quot;, &quot;normal_high&quot;, &quot;high&quot;. Defaults to normal.
                                        Note: &quot;normal_high&quot; is only available on DSP.
    --enable_cpu_fxp                    Enable the fixed point execution on CPU runtime.
    --userlogs=&lt;val&gt;                    Specifies the user level logging as level,&lt;optional logPath&gt;.
                                        Valid values are: &quot;warn&quot;, &quot;verbose&quot;, &quot;info&quot;, &quot;error&quot;, &quot;fatal&quot;
    --help                              Show this help message.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>