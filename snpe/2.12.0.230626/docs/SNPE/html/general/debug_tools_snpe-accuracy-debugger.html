

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Accuracy Debugger (Experimental) &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API" href="api.html" />
    <link rel="prev" title="Quantization Checker (Experimental)" href="debug_tools_snpe-quantization-checker.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.12.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="usergroup11.html">Debug Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="debug_tools_snpe-architecture-checker.html">Architecture Checker (Experimental)</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug_tools_snpe-quantization-checker.html">Quantization Checker (Experimental)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Accuracy Debugger (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="usergroup11.html">Debug Tools</a> &raquo;</li>
        
      <li>Accuracy Debugger (Experimental)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="accuracy-debugger-experimental">
<h1>Accuracy Debugger (Experimental)<a class="headerlink" href="#accuracy-debugger-experimental" title="Permalink to this headline">¶</a></h1>
<p><strong>Dependencies</strong></p>
<p>The Accuracy debugger depends on the environment setup and platform dependencies as outlined in the Setup page. User needs to run the envsetup.sh script and setup the ONNX, TensorFlow frameworks. User also needs to install the following pip packages before using the tool.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>absl==0.0
absl_py==0.13.0
flatbuffers==23.3.3
matplotlib==3.3.4
onnxruntime==1.14.1
protobuf==4.22.4
</pre></div>
</div>
<p>The following environment variables are used inside this guide (User may change the following paths depending on their needs):</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>RESOURCESPATH = {Path to the directory where all models and input files reside}</p></li>
<li><p>PROJECTREPOPATH = {Path to your accuracy debugger project directory}</p></li>
</ol>
</div></blockquote>
<p><strong>Overview</strong></p>
<p>The <strong>accuracy-debugger</strong> tool finds inaccuracies in a neural-network at the layer level. The tool compares the golden outputs produced by running a model through a specific ML framework (ie. Tensorflow, Onnx, TFlite) with the results produced by running the same model through Qualcomm’s SNPE Inference Engine. The inference engine can be run on a variety of computing mediums including GPU, CPU and DSP.</p>
<p>There are three stages in running the Accuracy Debugger. Each stage can be run using the binary with corresponding stage’s option like snpe-accuracy-debugger –{stage}. The three stages are as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>snpe-accuracy-debugger -–framework_diagnosis</strong> This stage uses frameworks ie. tensorflow, tflite and onnx to run the model to get intermediate outputs.</p></li>
<li><p><strong>snpe-accuracy-debugger –-inference_engine</strong> This stage uses SNPE engine to run the models to get intermediate outputs.</p></li>
<li><p><strong>snpe-accuracy-debugger –-verification</strong> This stage compares the output generated by framework diagnosis and inference engine stages using the verifiers like CosineSimilarity, RtolAtol, etc.</p></li>
</ol>
</div></blockquote>
<p>Tip: You can use -–help after the bin commands to see what other options (required or optional) you can add. To run all three options together just skip the -–{stage} option.</p>
<p>The following lists the steps required to execute the Accuracy Debugger tool:</p>
<p><strong>Framework Diagnosis</strong></p>
<p>The Framework diagnosis step is designed to run models with different machine learning frameworks (i.e. Tensorflow, etc). A selected model is run with a specific ML framework. Golden outputs are produced for future comparison with inference result.</p>
<p><strong>Usage</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: ./snpe-accuracy-debugger --framework_diagnosis [-h]
                                    -f FRAMEWORK [FRAMEWORK ...]
                                    -m MODEL_PATH
                                    -i INPUT_TENSOR [INPUT_TENSOR ...]
                                    -o OUTPUT_TENSOR
                                    [-w WORKING_DIR]
                                    [--output_dirname OUTPUT_DIRNAME]
                                    [-v]

Script to generate intermediate tensors from a ML Framework.

optional arguments:
     -h, --help            show this help message and exit

required arguments:
     -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                        Framework type and version, version is optional. Currently
                        supported frameworks are [&quot;tensorflow&quot;,&quot;onnx&quot;,&quot;tflite&quot;] case
                        insensitive but spelling sensitive
     -m MODEL_PATH, --model_path MODEL_PATH
                             Path to the model file(s).
     -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                             The name, dimensions, raw data, and optionally data
                             type of the network input tensor(s) specifiedin the
                             format &quot;input_name&quot; comma-separated-dimensions path-
                             to-raw-file, for example: &quot;data&quot; 1,224,224,3 data.raw
                             float32. Note that the quotes should always be
                             included in order to handle special characters,
                             spaces, etc. For multiple inputs specify multiple
                             --input_tensor on the command line like:
                             --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                             --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw float32.
     -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                             Name of the graph&#39;s specified output tensor(s).
     optional arguments:
     -w WORKING_DIR, --working_dir WORKING_DIR
                             Working directory for the framework_diagnosis to store
                             temporary files. Creates a new directory if the
                             specified working directory does not exist
     --output_dirname OUTPUT_DIRNAME
                             output directory name for the framework_diagnosis to
                             store temporary files under
                             &lt;working_dir&gt;/framework_diagnosis. Creates a new
                             directory if the specified working directory does not
                             exist
     -v, --verbose         Verbose printing
</pre></div>
</div>
<p>Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.</p>
<p><strong>Sample Commands</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./snpe-accuracy-debugger \
    --framework_diagnosis \
    --framework tensorflow \
    --model_path $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen.pb \
    --input_tensor &quot;input:0&quot; 1,299,299,3 $RESOURCESPATH/samples/InceptionV3Model/data/chairs.raw \
    --output_tensor InceptionV3/Predictions/Reshape_1:0

./snpe-accuracy-debugger \
    --framework_diagnosis \
    --framework onnx \
    --model_path $RESOURCESPATH/samples/dlv3onnx/dlv3plus_mbnet_513-513_op9_mod_basic.onnx \
    --input_tensor Input 1,3,513,513 $RESOURCESPATH/samples/dlv3onnx/data/00000_1_3_513_513.raw \
    --output_tensor Output
</pre></div>
</div>
<p>TIP:</p>
<ul class="simple">
<li><p>a working_directory, if not otherwise specified, is generated wherever you are calling the script from; it is recommended to call all scripts from the same directory so all your outputs and results are stored under the same directory without having outputs everywhere.</p></li>
<li><p>for tensorflow it is sometimes necessary to add the :0 after the input and output node name to signify the index of the node. Notice that the :0 is dropped for onnx models.</p></li>
</ul>
<p><strong>Output</strong></p>
<p>The program also creates a folder named latest found in working_directory/framework_diagnosis which is symbolic linked to the most recently generated directory. In the example below, latest will have data that is symbol linked to the data in the most recent directory YYYY-MM-DD_HH:mm:ss. User may choose to override the directory name by passing it to –output_dirname (i.e. –output_dirname myTest1Ouput).</p>
<p>The float data produced by Framework Diagnosis step offers precise reference material for the Verification component to diagnose the accuracy of the network generated by Inference Engine Diagnosis. Unless a path is otherwise specified, network diagnosis will create directories within the working_directory/framework_diagnosis folder found in the current working directory. The directories will be named with the date and time of the program’s execution, and contain tensor data. Depending on the tensor naming convention of the model, there may be numerous sub-directories within the new directory. This occurs when tensor names include a slash “/”. For example for tensor names ‘inception_3a/1x1/bn/sc’, ‘inception_3a/1x1/bn/sc_internal’ and ‘inception_3a/1x1/bn’, we would end up with subdirectories.</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/framework_diagnosis.png" src="../images/framework_diagnosis.png" />
</div>
</div>
<p>The figure above shows a sample output from one of the framework_diagnosis runs. Framework diagnosis basically runs inference at each op in the network. InceptionV3 and Logits contain the outputs of each layer before the last layer. Each output directory contains the .raw files corresponding to each node. Every raw file that can be seen is the output of an op. The outputs of the final layer are saved inside Predictions directory. The file framework_diagnosis_options.json contains all the options this stage was using.</p>
<p><strong>Step 2: Inference Engine</strong></p>
<p>The Inference Engine step is designed to find the outputs for a SNPE SDK compatible model. The output produced by this step can be compared with the Golden outputs produced by the framework diagnosis step.</p>
<p><strong>Usage</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: ./snpe-accuracy-debugger --inference_engine [-h]
                                 [--stage {source,converted,compiled}]
                                 -r {cpu,gpu,dsp,aic}
                                 -p ENGINE_PATH
                                 -a {aarch64-android,x86_64-linux-clang}
                                 -l INPUT_LIST
                                 [-i INPUT_TENSOR [INPUT_TENSOR ...]]
                                 [-o OUTPUT_TENSOR] [-m MODEL_PATH]
                                 [-f FRAMEWORK [FRAMEWORK ...]]
                                 [--static_model STATIC_MODEL]
                                 [--deviceId DEVICEID] [-v]
                                 [--host_device {x86}] [-w WORKING_DIR]
                                 [--output_dirname OUTPUT_DIRNAME]
                                 [--engine_version ENGINE_VERSION]
                                 [--debug_mode_off]
                                 [--print_version PRINT_VERSION]
                                 [--offline_prepare] [-bbw {8,32}]
                                 [-abw {8,16}]
                                 [--golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING]
                                 [-wbw {8,16}]
                                 [--fine_grain_mode FINE_GRAIN_MODE]
                                 [--no_weight_quantization]
                                 [--use_symmetric_quantize_weights]
                                 [--use_enhanced_quantizer]
                                 [--htp_socs HTP_SOCS]
                                 [--use_adjusted_weights_quantizer]
                                 [--override_params]

Script to run SNPE inference engine.

optional arguments:
-h, --help            show this help message and exit

Core Arguments:
--stage {source,converted,compiled}
                      Specifies the starting stage in the Accuracy Debugger
                      pipeline. Source: starting with a source framework model [default].
                      Compiled: starting with a model&#39;s .so binary
-r {cpu,gpu,dsp,aic}, --runtime {cpu,gpu,dsp,aic}
                      Runtime to be used.
-a {aarch64-android,x86_64-linux-clang}, --architecture {aarch64-android,x86_64-linux-clang}
                      Name of the architecture to use for inference engine.
-l INPUT_LIST, --input_list INPUT_LIST
                      Path to the input list text.

Arguments required for SOURCE stage:
-i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                      The name, dimension, and raw data of the network input
                      tensor(s) specified in the format &quot;input_name&quot; comma-
                      separated-dimensions path-to-raw-file, for example:
                      &quot;data&quot; 1,224,224,3 data.raw. Note that the quotes
                      should always be included in order to handle special
                      characters, spaces, etc. For multiple inputs specify
                      multiple --input_tensor on the command line like:
                      --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                      --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw.
-o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                      Name of the graph&#39;s output tensor(s).
-m MODEL_PATH, --model_path MODEL_PATH
                      Path to the model file(s).
-f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                      Framework type to be used, followed optionally by
                      framework version.

Arguments required for CONVERTED or COMPILED stage:
--static_model STATIC_MODEL
                      Path to the converted model.

Optional Arguments:
--deviceId DEVICEID   The serial number of the device to use. If not
                      available, the first in a list of queried devices will
                      be used for validation.
-v, --verbose         Verbose printing
--host_device {x86}   The device that will be running conversion. Set to x86
                      by default.
-p ENGINE_PATH, --engine_path ENGINE_PATH
                      Path to the inference engine.
-w WORKING_DIR, --working_dir WORKING_DIR
                      Working directory for the inference_engine to store
                      temporary files. Creates a new directory if the
                      specified working directory does not exist
--output_dirname OUTPUT_DIRNAME
                      output directory name for the inference_engine to
                      store temporary files under
                      &lt;working_dir&gt;/inference_engine. Creates a new
                      directory if the specified working directory does not
                      exist
--engine_version ENGINE_VERSION
                      engine version, will retrieve the latest available if
                      not specified
--debug_mode_off      Specifies if wish to turn off debug_mode mode.
--print_version PRINT_VERSION
                      Print the SNPE SDK version alongside the output.
--offline_prepare     Use offline prepare to run snpe model.
-bbw {8,32}, --bias_bitwidth {8,32}
                      option to select the bitwidth to use when quantizing
                      the bias. default 8
-abw {8,16}, --act_bitwidth {8,16}
                      option to select the bitwidth to use when quantizing
                      the activations. default 8
--golden_dir_for_mapping GOLDEN_DIR_FOR_MAPPING
                      Optional parameter to indicate the directory of the
                      goldens, it&#39;s used for tensor mapping without
                      framework.
-wbw {8,16}, --weights_bitwidth {8,16}
                      option to select the bitwidth to use when quantizing
                      the weights. default 8
--fine_grain_mode FINE_GRAIN_MODE
                      Path to the model golden outputs required to run
                      inference engine using fine-grain mode.
--no_weight_quantization
                      Generate and add the fixed-point encoding metadata but
                      keep the weights in floating point
--use_symmetric_quantize_weights
                      Use the symmetric quantizer feature when quantizing
                      the weights of the model
--use_enhanced_quantizer
                      Use the enhanced quantizer feature when quantizing the
                      model
--htp_socs HTP_SOCS   Specify SoC to generate HTP Offline Cache for.
--use_adjusted_weights_quantizer
                      Use the adjusted tf quantizer for quantizing the
                      weights only
--override_params     Use this option to override quantization parameters
                      when quantization was provided from the original
                      source framework
</pre></div>
</div>
<p>Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.</p>
<p>The inference engine config file can be found in {accuracy_debugger tool root directory}/python/qti/aisw/accuracy_debugger/lib/inference_engine/configs/config_files and is a JSON file. This config file stores information that helps the inference engine know which tool and parameters to read in. Each different inference engine, and possibly engine versions in certain cases, will require its own config file.</p>
<p><strong>Sample Command</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./snpe-accuracy-debugger \
    --inference_engine \
    --framework tensorflow \
    --runtime cpu \
    --model_path $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen.pb \
    --input_tensor &quot;input:0&quot; 1,299,299,3 $RESOURCESPATH/samples/InceptionV3Model/data/chairs.raw \
    --output_tensor InceptionV3/Predictions/Reshape_1:0 \
    --architecture x86_64-linux-clang \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --verbose
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./snpe-accuracy-debugger \
    --inference_engine \
    --deviceId 357415c4 \
    --framework tensorflow \
    --soc_name &#39;kailua&#39; \
    --runtime dsp \
    --architecture aarch64-android \
    --model_path $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen.pb \
    --input_tensor &quot;input:0&quot; 1,299,299,3 $RESOURCESPATH/samples/InceptionV3Model/data/chairs.raw \
    --output_tensor InceptionV3/Predictions/Reshape_1:0 \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --verbose
</pre></div>
</div>
<p>Tip:</p>
<ul class="simple">
<li><p>for –runtime (-r) dsp is used for snpe-net-run</p></li>
<li><p>the input_tensor (–i) and output_tensor (-o) does not need the :0 indexing like when runing tensorflow framework diagnosis</p></li>
<li><p>two files, namely tensor_mapping.json and snpe_model_graph_struct.json are generated to be used in verification, be sure to locate these 2 files in the working_directory/inference_engine/latest</p></li>
<li><p>framework and golden_dir_for_mapping, or just golden_dir_for_mapping itself is an alternative to the original model to be provided to generate the tensor_mapping.json, however, providing only the golden_dir_for_mapping, the get_tensor_mapping module will try it’s best to map but it is not guaranteed the mapping would be 100% accurate.</p></li>
</ul>
<p><strong>Output</strong></p>
<p>Once the inference engine has finished running, it will store the output in the specified directory (or the current working directory by default) and store the files in that folder. By default, it will store the output in working_directory/inference_engine in the current working directory.</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/inference_engine_1.png" src="../images/inference_engine_1.png" />
</div>
</div>
<p>The figure above shows the sample output from one of the runs of inference engine step. The output directory contains raw files. Each raw file is an output of an op involved in various layers in the network. File input_list.txt and the directory input_list_files contain the path for sample test images. inference_engine_options.json contains all the options with which this run was launched. In addition to generating the .raw files, inference_engine also generates the model’s graph structure in a .json file. The name of the file is the same as the name of the protobuf model file. The file base.dlc is the converted model graph. The file model_graph_struct.json aids in providing the structure related information of the converted model graph during the verification stage. Specifically, it helps with organizing the nodes in order (for e.g. the beginning nodes should come earlier than ending nodes). It helps to see the nodes in a streamlined manner. Finally, tensor_mapping json file contains a mapping of the various intermediate output file names generated from the framework diagnosis step and the inference engine step.</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/inference_engine_2.png" src="../images/inference_engine_2.png" />
</div>
</div>
<p>The created .raw files are organized in the same manner as framework_diagnosis (see above).</p>
<p><strong>Step 3: Verification</strong></p>
<p>The Verification step compares the output (from the intermediate tensors of a given model) produced by framework diagonsis step with the one that’s produced by the inference engine step. Once the comparison is complete the verification results are compiled and displayed visually in a format that can be easily interpreted by the user.</p>
<p>There are different types of verifiers for e.g.: CosineSimilarity, RtolAtol, etc. To see what all verifiers are there please use the –help option like ./snpe_accuracy_debugger –verification –help. Each verifier compares the Framework Diagnosis and Inference Engine output using an error metric. It also prepares reports and/or visualizations to help the user analyze the network’s error data.</p>
<p><strong>Usage</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: ./snpe-accuracy-debugger --verification [-h]

Script to run verification.

required arguments:
    --default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                           Default verifier used for verification. The options
                           &quot;RtolAtol&quot;, &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;,
                           &quot;CosineSimilarity&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;,
                           &quot;ScaledDiff&quot; are supported. An optional list of
                           hyperparameters can be appended. For example:
                           --default_verifier
                           rtolatol,rtolmargin,0.01,atolmargin,0,01. An optional
                           list of placeholders can be appended. For example:
                           --default_verifier CosineSimilarity param1 1 param2 2.
                           to use multiple verifiers, add additional
                           --default_verifier CosineSimilarity
    --framework_results FRAMEWORK_RESULTS
                           Path to root directory generated from framework
                           diagnosis. Paths may be absolute, or relative to the
                           working directory.
    --inference_results INFERENCE_RESULTS
                           Path to root directory generated from inference engine
                           diagnosis. Paths may be absolute, or relative to the
                           working directory.

optional arguments:
    --tensor_mapping TENSOR_MAPPING
                           Path to the file describing the tensor name mapping
                           between inference and golden tensors.can be generated
                           with nd_run_{engine}_inference_engine
    --verifier_config VERIFIER_CONFIG
                           Path to the verifiers&#39; config file
    --graph_struct GRAPH_STRUCT
                           Path to the inference graph structure .json file.
    -v, --verbose         Verbose printing
    -w WORKING_DIR, --working_dir WORKING_DIR
                           Working directory for the verification to store
                           temporary files. Creates a new directory if the
                           specified working directory does not exist
    --output_dirname OUTPUT_DIRNAME
                           output directory name for the verification to store
                           temporary files under &lt;working_dir&gt;/verification.
                           Creates a new directory if the specified working
                           directory does not exist

arguments for generating a new tensor_mapping.json:
    -m MODEL_PATH, --model_path MODEL_PATH
                           path to original model for tensor_mapping uses here.
    -e ENGINE_NAME [ENGINE_VERSION ...], --engine ENGINE_NAME [ENGINE_VERSION ...]
                           Name of engine that will be running inference,
                           optionally followed by the engine version. Used here
                           for tensor_mapping.
    -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                           Framework type to be used, followed optionally by
                           framework version. Used here for tensor_mapping.
</pre></div>
</div>
<p>Please note: All the command line arguments should either be provided through command line or through the config file. They will not override those in the config file if there is overlap.</p>
<p>The main verification process run using ./snpe-accuracy-debugger -–verification optionally uses –tensor_mapping and -–graph_struct to find files to compare. These files are generated by the inference engine step, and should be supplied to verification for best results. By default they are named tensor_mapping.json and {model name}_graph_struct.json, and can be found in the output directory of the inference engine results.</p>
<p><strong>Sample Command</strong></p>
<p>Compare output of framework diagnosis with inference engine:</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./snpe-accuracy-debugger \
    --verification \
    --default_verifier CosineSimilarity param1 1 param2 2 \
    --default_verifier SQNR param1 5 param2 1 \
    --framework_results $PROJECTREPOPATH/working_directory/framework_diagnosis/2022-10-31_17-07-58/ \
    --inference_results $PROJECTREPOPATH/working_directory/inference_engine/latest/output/Result_0/ \
    --tensor_mapping $PROJECTREPOPATH/working_directory/inference_engine/latest/tensor_mapping.json \
    --graph_struct $PROJECTREPOPATH/working_directory/inference_engine/latest/snpe_model_graph_struct.json
</pre></div>
</div>
<p>Tip:</p>
<ul class="simple">
<li><p>If you passed multiple images in the image_list.txt from run inference engine diagnosis, you’ll receive multiple output/Result_x, choose result that matches the input you used for framework diagnosis for comparison (ie. in framework you used chair.raw and inference chair.raw was the first item in the image_list.txt then choose output/Result_0, if chair.raw was the second item in image_list.txt, then choose output/Result_1).</p></li>
<li><p>It is recommended to always supply –graph_struct and –tensor_mapping to the command as it is used to line up the report and find the corresponding files for comparison. if –tensor_mapping did not get generated from previous steps, you can supplement with –model_path, –engine, –framework to have module generate tensor_mapping during runtime.</p></li>
<li><p>You can also compare inference_engine outputs to inference_engine outputs by passing the /output of the inference_engine output to the –framework_results. If you want the outputs to be exact-name-matching, then you do not need to provide a tensor_mapping file.</p></li>
<li><p>Note that if you need to generate a tensor mapping instead of providing a path to pre-existing tensor mapping file. You can provide the –model_path option.</p></li>
</ul>
<p>Verifier uses two optional config files. The first file is used to prefer parameters to specific verifiers, as well as which tensors to use these verifiers on. The second file is used to map tensor names from inference_engine to framework_diagnosis, since certain tensors generated by framework_diagnosis have different names than tensors generated by inference_engine.</p>
<p><strong>Verifier Config:</strong></p>
<p>The verifier config file is a JSON file that tells verification which verifiers (asides from the default verifier) to use and with which parameters and on what specific tensors. If no config file is provided, the tool will only use the default verifier specified from the command line, with its default parameters, on all the tensors. The JSON file is keyed by verifier names, with each verifier as its own dictionary keyed by “parameters” and “tensors”.</p>
<p><strong>Config File</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>```json
{
  &quot;MeanIOU&quot;: {
      &quot;parameters&quot;: {
          &quot;background_classification&quot;: 1.0
      },
      &quot;tensors&quot;: [[&quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;, &quot;detection_classes:0&quot;]]
  },
  &quot;TopK&quot;: {
      &quot;parameters&quot;: {
          &quot;k&quot;: 5,
          &quot;ordered&quot;: false
      },
      &quot;tensors&quot;: [[&quot;Reshape_1:0&quot;], [&quot;detection_classes:0&quot;]]
  }
}
```
</pre></div>
</div>
<p>Note that the “tensors” field is a list of lists. This is done because specific verifiers (e.g. MeanIOU) runs on two tensor at a time. Hence the two tensors are placed in a list. Otherwise if a verifier only runs on one tensor, it will have a list of lists with only one tensor name in each list.</p>
<p><strong>Tensor Mapping:</strong></p>
<p>Tensor mapping is a JSON file keyed by inference tensor names, of framework tensor names. If the tensor mapping is not provided, the tool will assume inference and golden tensor names are identical.</p>
<p><strong>Tensor Mapping File</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>```json
{
  &quot;Postprocessor/BatchMultiClassNonMaxSuppression_boxes&quot;: &quot;detection_boxes:0&quot;,
  &quot;Postprocessor/BatchMultiClassNonMaxSuppression_scores&quot;: &quot;detection_scores:0&quot;
}
```
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>Verification’s output is divided into different verifiers. For example, if both RtolAtol and TopK verifiers are used, there will be two sub-folders named “RtolAtol” and “TopK”. Availble verifiers can be found by just issuing –help option.</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/verification_1.png" src="../images/verification_1.png" />
</div>
</div>
<p>Under each sub-folder, the verification analysis for each tensor is organized similar to how framework_diagnosis and inference_engine (see above) are organized. For each tensor, a CSV and HTML file is generated. In addition to the tensor-specific analysis, the tool also generates a summary CSV and HTML file which summarizes the data from all verifiers and their subsequent tensors. The following figure shows how a sample summary generated in the verification step looks. Each row in this summary corresponds to one tensor name that is identified by the framework diagnosis and inference engine steps. The final column shows cosinesimilarity score which can vary between 0 to 1 (this range might be different for other verifiers). If the score is high enough then it means that the result produced by both the steps for that particular tensor are fairly similar. However, if the score is too low then it gives the developer a point of inspection. The developer can then further investigate those specific tensors (if multiple) into details. Developer should inspect tensors from top-to-bottom order, meaning if a tensor is broken at an earlier node, anything that was generated post that node is unreliable until that node is properly fixed. Hence, this process might help in improving the overall accuracy of the sdk. That is how this tool helps in debugging.</p>
<div class="docutils container">
<div class="figure align-default">
<img alt="../images/verification_2.png" src="../images/verification_2.png" />
</div>
</div>
<p><strong>Run SNPE Accuracy Debugger E2E:</strong></p>
<p>This feature is designed to run all three steps namely framework diagnosis, inference engine and verification sequentially with a single command.</p>
<p><strong>Usage</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>usage: ./snpe-accuracy-debugger [--framework_diagnosis] [--inference_engine] [--verification] [-h]

Script that runs Framework Diagnosis, Inference Engine or Verification.

Arguments to select which component of the tool to run.  Arguments are mutually exclusive (at most 1 can be selected).  If none are selected, then all components are run:
--framework_diagnosis Run framework
--inference_engine    Run inference engine
--verification        Run verification

optional arguments:
-h, --help            Show this help message. To show help for any of the
                      components, run script with --help and --&lt;component&gt;.
                      For example, to show the help for Framework Diagnosis,
                      run script with the following: --help
                      --framework_diagnosis
usage: ./snpe-accuracy-debugger [-h] -f FRAMEWORK [FRAMEWORK ...] -m MODEL_PATH
                          -i INPUT_TENSOR [INPUT_TENSOR ...] -o
                          OUTPUT_TENSOR -r RUNTIME -a
                          {aarch64-android,x86_64-linux-clang}
                          -l INPUT_LIST --default_verifier DEFAULT_VERIFIER
                          [DEFAULT_VERIFIER ...] [-v] [-w WORKING_DIR]
                          [--output_dirname OUTPUT_DIRNAME]
                          [--deep_analyzer {modelDissectionAnalyzer}]

Options for running the Accuracy Debugger components

optional arguments:
-h, --help            show this help message and exit

Arguments required by both Framework Diagnosis and Inference Engine:
   -f FRAMEWORK [FRAMEWORK ...], --framework FRAMEWORK [FRAMEWORK ...]
                          Framework type and version, version is optional.
   -m MODEL_PATH, --model_path MODEL_PATH
                          Path to the model file(s).
   -i INPUT_TENSOR [INPUT_TENSOR ...], --input_tensor INPUT_TENSOR [INPUT_TENSOR ...]
                          The name, dimensions, raw data, and optionally data
                          type of the network input tensor(s) specifiedin the
                          format &quot;input_name&quot; comma-separated-dimensions path-
                          to-raw-file, for example: &quot;data&quot; 1,224,224,3 data.raw
                          float32. Note that the quotes should always be
                          included in order to handle special characters,
                          spaces, etc. For multiple inputs specify multiple
                          --input_tensor on the command line like:
                          --input_tensor &quot;data1&quot; 1,224,224,3 data1.raw
                          --input_tensor &quot;data2&quot; 1,50,100,3 data2.raw float32.
   -o OUTPUT_TENSOR, --output_tensor OUTPUT_TENSOR
                          Name of the graph&#39;s specified output tensor(s).

Arguments required by Inference Engine:
-r RUNTIME, --runtime RUNTIME
                      Runtime to be used for inference.
-a {aarch64-android,x86_64-linux-clang}, --architecture {aarch64-android,x86_64-linux-clang}
                      Name of the architecture to use for inference engine.
-l INPUT_LIST, --input_list INPUT_LIST
                      Path to the input list text.

Arguments required by Verification:
--default_verifier DEFAULT_VERIFIER [DEFAULT_VERIFIER ...]
                      Default verifier used for verification. The options
                      &quot;RtolAtol&quot;, &quot;AdjustedRtolAtol&quot;, &quot;TopK&quot;, &quot;L1Error&quot;,
                      &quot;CosineSimilarity&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;SQNR&quot;, &quot;MeanIOU&quot;,
                      &quot;ScaledDiff&quot; are supported. An optional list of
                      hyperparameters can be appended. For example:
                      --default_verifier
                      rtolatol,rtolmargin,0.01,atolmargin,0,01. An optional
                      list of placeholders can be appended. For example:
                      --default_verifier CosineSimilarity param1 1 param2 2.
                      to use multiple verifiers, add additional
                      --default_verifier CosineSimilarity

optional arguments:
-v, --verbose         Verbose printing
-w WORKING_DIR, --working_dir WORKING_DIR
                      Working directory for the wrapper to store temporary
                      files. Creates a new directory if the specified
                      working directory does not exitst.
--output_dirname OUTPUT_DIRNAME
                      output directory name for the wrapper to store
                      temporary files under &lt;working_dir&gt;/wrapper. Creates a
                      new directory if the specified working directory does
                      not exist
--deep_analyzer {modelDissectionAnalyzer}
                      Deep Analyzer to perform deep analysis
</pre></div>
</div>
<p><strong>Sample Command</strong></p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>./snpe-accuracy-debugger \
    --framework tensorflow \
    --runtime cpu \
    --model_path $RESOURCESPATH/samples/InceptionV3Model/inception_v3_2016_08_28_frozen.pb \
    --input_tensor &quot;input:0&quot; 1,299,299,3 $RESOURCESPATH/samples/InceptionV3Model/data/chairs.raw \
    --output_tensor InceptionV3/Predictions/Reshape_1:0 \
    --architecture x86_64-linux-clang \
    --input_list $RESOURCESPATH/samples/InceptionV3Model/data/image_list.txt \
    --default_verifier CosineSimilarity \
    --framework_results $PROJECTREPOPATH/working_directory/framework_diagnosis/latest/ \
    --inference_results $PROJECTREPOPATH/working_directory/inference_engine/latest/output/Result_0/ \
    --tensor_mapping $PROJECTREPOPATH/working_directory/inference_engine/latest/tensor_mapping.json \
    --graph_struct $PROJECTREPOPATH/working_directory/inference_engine/latest/model_graph_struct_graph_struct.json \
    --verbose
</pre></div>
</div>
<p><strong>Output</strong></p>
<p>The program creates the respective output directories/files as discussed in framework diagnosis, inference engine and verification.</p>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="api.html" class="btn btn-neutral float-right" title="API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="debug_tools_snpe-quantization-checker.html" class="btn btn-neutral float-left" title="Quantization Checker (Experimental)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>