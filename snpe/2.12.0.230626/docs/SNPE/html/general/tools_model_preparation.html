

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>snpe-dlc-graph-prepare &mdash; Snapdragon Neural Processing Engine SDK</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_css.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Qualcomm® Neural Processing SDK
          

          
          </a>

          
            
            
              <div class="version">
                v2.12.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="revision_history.html">Revision History</a></li>
<li class="toctree-l1"><a class="reference internal" href="revision_history_windows.html">Revision History - Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup1.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup5.html">Input Data and Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup6.html">Tutorials and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup10.html">Benchmarking and Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="usergroup11.html">Debug Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="appx_ref.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® Neural Processing SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>snpe-dlc-graph-prepare</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="snpe-dlc-graph-prepare">
<h1>snpe-dlc-graph-prepare<a class="headerlink" href="#snpe-dlc-graph-prepare" title="Permalink to this headline">¶</a></h1>
<p>snpe-dlc-graph-prepare is used to perform offline graph
preparation on quantized dlcs to run on DSP/HTP runtimes.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
  [ -h, --help ]        Displays this help message.
  [ --version ]         Displays version information.
  [ --verbose ]         Enable verbose user messages.
  [ --quiet ]           Disables some user messages.
  [ --silent ]          Disables all but fatal user messages.
  [ --debug=&lt;val&gt; ]     Sets the debug log level.
  [ --debug1 ]          Enables level 1 debug messages.
  [ --debug2 ]          Enables level 2 debug messages.
  [ --debug3 ]          Enables level 3 debug messages.
  [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
  [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
  [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
  [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
  --input_dlc=&lt;val&gt;     Path to the dlc container containing the model for which graph cache
                        should be generated. This argument is required.
  [ --output_dlc=&lt;val&gt; ]
                        Path at which the cached data included model container should be written.
                        If this argument is omitted, the quantized model will be written at
                        &lt;input_model_name&gt;_cached.dlc.
  [ --set_output_tensors=&lt;val&gt; ]
                        Specifies a comma separated list of tensors to be output after execution
                        without whitespace.
  [ --set_output_layers=&lt;val&gt; ]
                        Specifies a comma separated list of layers whose output buffers should be
                        output after execution, without whitespace.
  [ --input_list=&lt;val&gt; ]
                        Path to a file specifying input images as passed to snpe-net-run. Only
                        the graph output buffers information specified in the input list (line starting
                        with # or %, if any) will be used. Paths to the input images will be ignored
  [ --htp_socs=&lt;val&gt; ]  Specify SoC(s) to generate HTP Offline Cache for. SoCs are specified with an
                        ASIC identifier, in a comma seperated list without whitespace. For example
                        --htp_socs sm8350,sm8450,sm8550. This flag and --htp_archs are mutually exclusive.
                        Default ASIC identifier: sm8550
  [ --htp_archs=&lt;val&gt; ]
                        Specify DSP Architecture(s) to generate general HTP Offline Cache for.
                        Architectures are specified with an ASIC identifier, in a comma seperated list
                        without whitespace. For example, --htp_archs v68,v73. This flag cannot be
                        coupled with --htp_socs or --vtcm_override
  [ --vtcm_override=&lt;val&gt; ]
                        Specify a single value representing the VTCM size in MB for the generated HTP Offline Caches.
                        For example, --vtcm_override 4. This flag can be used in conjunction with --htp_socs to
                        override the default SOC vtcm size setting
  [ --buffer_data_type=&lt;val&gt; ]
                        Sets data type of IO buffers during prepare. Data Type can be the following:
                        float32, fixedPoint8, fixedPoint16. Arguments should be formatted as follows:
                        --buffer_data_type buffer_name1=buffer_name1_data_type
                        --buffer_data_type buffer_name2=buffer_name2_data_type
                        (Note: deprecated)
  [ --overwrite_cache_records ]
                        Erase all HTP cache records present in the DLC before generating requested caches
  [ --use_float_io ]    Prepare quantized HTP Graph to operate with floating point inputs/outputs (Note: deprecated)
  [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for UDO Package(s). Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        Optionally, user can provide multiple packages as a comma-separated list.
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host executable CPU Implementation
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-dlc-quant">
<h1>snpe-dlc-quant<a class="headerlink" href="#snpe-dlc-quant" title="Permalink to this headline">¶</a></h1>
<p>snpe-dlc-quant converts non-quantized DLC models into quantized
DLC models.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
    [ -h,--help ]         Displays this help message.
    [ --version ]         Displays version information.
    [ --verbose ]         Enable verbose user messages.
    [ --quiet ]           Disables some user messages.
    [ --silent ]          Disables all but fatal user messages.
    [ --debug=&lt;val&gt; ]     Sets the debug log level.
    [ --debug1 ]          Enables level 1 debug messages.
    [ --debug2 ]          Enables level 2 debug messages.
    [ --debug3 ]          Enables level 3 debug messages.
    [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
    [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
    [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
    [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
    [ --input_dlc=&lt;val&gt; ]
                        Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
    [ --input_list=&lt;val&gt; ]
                        Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the &#39;raw&#39; format, ready to be consumed by the tool without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
    [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
    [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
    [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized.
    [ --use_adjusted_weights_quantizer ]
                        Use the adjusted tf quantizer for quantizing the weights only. This might be helpful for improving the accuracy of some models,
                        such as denoise model as being tested. This option is only used when quantizing the weights with 8 bit.
    [ --optimizations ]   Use this option to enable new optimization algorithms. Usage is:
                        --optimizations &lt;algo_name1&gt; --optimizations &lt;algo_name2&gt;
                        The available optimization algorithms are:
                        cle - Cross layer equalization includes a number of methods for equalizing weights and biases across layers in order to rectify imbalances that cause quantization errors.
    [ --override_params ]
                        Use this option to override quantization parameters when quantization was provided from the original source framework (eg TF fake quantization)
    [ --use_encoding_optimizations ]
                        Use this option to enable quantization encoding optimizations. This can reduce requantization in the graph and may improve accuracy for some models
                        (Note: deprecated).
    [ --use_symmetric_quantize_weights ]
                        Use the symmetric quantizer feature when quantizing the weights of the model. It makes sure min and max have the
                        same absolute values about zero. Symmetrically quantized data will also be stored as int#_t data such that the offset is always 0.
    [ --use_native_dtype ]
                        Note: This option is deprecated, use --use_native_input_files option in future.
                          Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_input_files ]
                        Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_output_files ]
                        Use this option to indicate the data type of the output files,
                           1. float (default): generates the output file as float data.
                           2. native:          generates the output file as datatype native to the source model. i.e. uint8_t.
    [ --bias_bitwidth=&lt;val&gt; ]
                        Use the --bias_bitdwith option to select the bitwidth to use when quantizing the biases, either 8 (default) or 32. Using 32 bit biases may
                        sometimes provide a small improvement in accuracy. Can&#39;t mix with --bitwidth.
    [ --float_bitwidth=&lt;val&gt; ]
                        Use the --float_bitwidth option to select the bitwidth to use when using float
                        for parameters(weights/bias) and activations for all ops or specific
                        Op (via encodings) selected through encoding, either 32 (default) or 16.
    [ --act_bitwidth=&lt;val&gt; ]
                        Use the --act_bitwidth option to select the bitwidth to use when quantizing the activations, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --weights_bitwidth=&lt;val&gt; ]
                        Use the --weights_bitwidth option to select the bitwidth to use when quantizing the weights, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --bitwidth=&lt;val&gt; ]
                        Use the --bitwidth option to select the bitwidth to use when quantizing the weights/activation/bias, either 8 (default) or 16. Can&#39;t mix with
                        --weights_bitwidth or --act_bitwidth or --bias_bitdwith.
    [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for a UDO Package. Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host-executable CPU Implementation
    [ --axis_quant ]
                        Use the --axis_quant option to select per-axis-element quantization for the weights and biases of certain layer types.
                        Currently only Convolution, Deconvolution and FullyConnected are supported.


Description:
Generate 8 or 16 bit TensorFlow style fixed point weight and activations encodings for a floating point DLC.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p>For specifying input_list, refer to input_list
argument in
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
supported input formats (in order to calculate output
activation encoding information for all layers, <strong>do not</strong>
include the line which specifies desired outputs).</p></li>
<li><p>The tool requires the batch dimension of the DLC input file
to be set to 1 during the original model conversion step.</p>
<blockquote>
<div><ul>
<li><p>An example of quantization using snpe-dlc-quant can be found
in the C/C++ Tutorial section: <a class="reference external" href="tutorial_inceptionv3.html">Running the Inception v3
Model</a>. For details on
quantization see <a class="reference external" href="quantized_models.html">Quantized vs Non-Quantized
Models</a>.</p></li>
<li><p>Outputs can be specified for snpe-dlc-quant by modifying the
input_list in the following ways:</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified
individually, but when specifying both, the order shown must
be used to specify each.</p>
</div></blockquote>
</li>
<li><p>When using the Qualcomm® Neural Processing SDK API:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quant was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputLayers()</span>
function.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quant was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputTensors()</span>
function.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-dlc-quantize">
<h1>snpe-dlc-quantize<a class="headerlink" href="#snpe-dlc-quantize" title="Permalink to this headline">¶</a></h1>
<p>snpe-dlc-quantize converts non-quantized DLC models into
quantized DLC models.</p>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>Command Line Options:
    [ -h,--help ]         Displays this help message.
    [ --version ]         Displays version information.
    [ --verbose ]         Enable verbose user messages.
    [ --quiet ]           Disables some user messages.
    [ --silent ]          Disables all but fatal user messages.
    [ --debug=&lt;val&gt; ]     Sets the debug log level.
    [ --debug1 ]          Enables level 1 debug messages.
    [ --debug2 ]          Enables level 2 debug messages.
    [ --debug3 ]          Enables level 3 debug messages.
    [ --log-mask=&lt;val&gt; ]  Sets the debug log mask to set the log level for one or more areas.
                        Example: &quot;.*=USER_ERROR, .*=INFO, NDK=DEBUG2, NCC=DEBUG3&quot;
    [ --log-file=&lt;val&gt; ]  Overrides the default name for the debug log file.
    [ --log-dir=&lt;val&gt; ]   Overrides the default directory path where debug log files are written.
    [ --log-file-include-hostname ]
                        Appends the name of this host to the log file name.
    [ --input_dlc=&lt;val&gt; ]
                        Path to the dlc container containing the model for which fixed-point encoding
                        metadata should be generated. This argument is required.
    [ --input_list=&lt;val&gt; ]
                        Path to a file specifying the trial inputs. This file should be a plain text file,
                        containing one or more absolute file paths per line. These files will be taken to constitute
                        the trial set. Each path is expected to point to a binary file containing one trial input
                        in the &#39;raw&#39; format, ready to be consumed by the tool without any further modifications.
                        This is similar to how input is provided to snpe-net-run application.
    [ --no_weight_quantization ]
                        Generate and add the fixed-point encoding metadata but keep the weights in
                        floating point. This argument is optional.
    [ --output_dlc=&lt;val&gt; ]
                        Path at which the metadata-included quantized model container should be written.
                        If this argument is omitted, the quantized model will be written at &lt;unquantized_model_name&gt;_quantized.dlc.
    [ --enable_htp ]      Pack HTP information in quantized DLC.
    [ --htp_socs=&lt;val&gt; ]  Specify SoC to generate HTP Offline Cache for.
                        SoCs are specified with an ASIC identifier, in a comma separated list.
                        For example, --htp_socs sm8550
    [ --overwrite_cache_records ]
                        Overwrite HTP cache records present in the DLC.
    [ --use_float_io ]
                        Pack HTP information in quantized DLC (Note: deprecated).
    [ --use_enhanced_quantizer ]
                        Use the enhanced quantizer feature when quantizing the model.  Regular quantization determines the range using the actual
                        values of min and max of the data being quantized.  Enhanced quantization uses an algorithm to determine optimal range.  It can be
                        useful for quantizing models that have long tails in the distribution of the data being quantized.
    [ --use_adjusted_weights_quantizer ]
                        Use the adjusted tf quantizer for quantizing the weights only. This might be helpful for improving the accuracy of some models,
                        such as denoise model as being tested. This option is only used when quantizing the weights with 8 bit.
    [ --optimizations ]   Use this option to enable new optimization algorithms. Usage is:
                        --optimizations &lt;algo_name1&gt; --optimizations &lt;algo_name2&gt;
                        The available optimization algorithms are:
                        cle - Cross layer equalization includes a number of methods for equalizing weights and biases across layers in order to rectify imbalances that cause quantization errors.
                        bc - Bias correction adjusts biases to offset activation quantization errors. Typically used in conjunction with &#39;cle&#39; to improve quantization accuracy (Note: deprecated).
    [ --override_params ]
                        Use this option to override quantization parameters when quantization was provided from the original source framework (eg TF fake quantization)
    [ --use_encoding_optimizations ]
                        Use this option to enable quantization encoding optimizations. This can reduce requantization in the graph and may improve accuracy for some models
                        (Note: this flag can be passed in, but is a no-op. Recognition of this flag will be removed in the future).
    [ --use_symmetric_quantize_weights ]
                        Use the symmetric quantizer feature when quantizing the weights of the model. It makes sure min and max have the
                        same absolute values about zero. Symmetrically quantized data will also be stored as int#_t data such that the offset is always 0.
    [ --use_native_dtype ]
                        Note: This option is deprecated, use --use_native_input_files option in future.
                          Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_input_files ]
                        Use this option to indicate how to read input files,
                           1. float (default): reads inputs as floats and quantizes if necessary based on quantization parameters in the model.
                           2. native:          reads inputs assuming the data type to be native to the model. For ex., uint8_t.
    [ --use_native_output_files ]
                        Use this option to indicate the data type of the output files,
                           1. float (default): generates the output file as float data.
                           2. native:          generates the output file as datatype native to the source model. i.e. uint8_t.
    [ --bias_bitwidth=&lt;val&gt; ]
                        Use the --bias_bitdwith option to select the bitwidth to use when quantizing the biases, either 8 (default) or 32. Using 32 bit biases may
                        sometimes provide a small improvement in accuracy. Can&#39;t mix with --bitwidth.
    [ --float_bitwidth=&lt;val&gt; ]
                        Use the --float_bitwidth option to select the bitwidth to use when using float
                        for parameters(weights/bias) and activations for all ops or specific
                        Op (via encodings) selected through encoding, either 32 (default) or 16.
    [ --act_bitwidth=&lt;val&gt; ]
                        Use the --act_bitwidth option to select the bitwidth to use when quantizing the activations, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --weights_bitwidth=&lt;val&gt; ]
                        Use the --weights_bitwidth option to select the bitwidth to use when quantizing the weights, either 8 (default) or 16. 8w/16a is only supported
                        by the HTA currently. Can&#39;t mix with --bitwidth.
    [ --bitwidth=&lt;val&gt; ]
                        Use the --bitwidth option to select the bitwidth to use when quantizing the weights/activation/bias, either 8 (default) or 16. Can&#39;t mix with
                        --weights_bitwidth or --act_bitwidth or --bias_bitdwith.
    [ --udo_package_path=&lt;val&gt; ]
                        Use this option to specify path to the Registration Library for a UDO Package. Usage is:
                        --udo_package_path=&lt;path_to_reg_lib&gt;
                        This option must be specified for Networks with UDO. All UDO&#39;s in Network must have host-executable CPU Implementation
    [ --axis_quant ]
                        Use the --axis_quant option to select per-axis-element quantization for the weights and biases of certain layer types.
                        Currently only Convolution, Deconvolution and FullyConnected are supported.


Description:
Generate 8 or 16 bit TensorFlow style fixed point weight and activations encodings for a floating point DLC model.
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
<div class="line">Additional details:</div>
</div>
<ul>
<li><p>For specifying input_list, refer to input_list
argument in
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
supported input formats (in order to calculate output
activation encoding information for all layers, <strong>do not</strong>
include the line which specifies desired outputs).</p></li>
<li><p>The tool requires the batch dimension of the DLC input file
to be set to 1 during the original model conversion step.</p></li>
<li><p>An example of quantization using snpe-dlc-quantize can be
found in the C/C++ Tutorial section: <a class="reference external" href="tutorial_inceptionv3.html">Running the Inception
v3 Model</a>. For details on
quantization see <a class="reference external" href="quantized_models.html">Quantized vs Non-Quantized
Models</a>.</p></li>
<li><p>Using snpe-dlc-quantize is mandatory for running on HTA.</p></li>
<li><p>Using snpe-dlc-quantize is mandatory for running on DSP
runtime on Snapdragon 865. It is recommended that offline
cache generation be used. It is specified by using
<strong>–enable_htp</strong> option for snpe-dlc-quantize.</p></li>
<li><p>When using offline cache generation for HTP, the same
input(s) tensors or layers and output(s) tensors or layers
should be specified when using snpe-dlc-quantize and to run
inference on the model using Qualcomm® Neural Processing SDK APIs or snpe-net-run. Not
doing so will cause the cache to be invalidated, and graph
initialization will take longer.</p></li>
<li><p>Outputs can be specified for snpe-dlc-quantize by modifying
the input_list in the following ways:</p>
<blockquote>
<div><div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>#&lt;output_layer_name&gt;[&lt;space&gt;&lt;output_layer_name&gt;]
%&lt;output_tensor_name&gt;[&lt;space&gt;&lt;output_tensor_name&gt;]
&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;[&lt;space&gt;&lt;input_layer_name&gt;:=&lt;input_layer_path&gt;]
…
</pre></div>
</div>
<p><strong>Note:</strong> Output tensors and layers can be specified
individually, but when specifying both, the order shown must
be used to specify each.</p>
</div></blockquote>
</li>
<li><p>When running a model with an offline generated cache using
snpe-net-run:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quantize was
called, need to be specified using the input list as
shown in the input_list argument to
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a>.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quantized was
called, need to be specified using the
<em>–set_output_tensors</em> argument to snpe-net-run. Refer to
<a class="reference external" href="tools.html#snpe-net-run">snpe-net-run</a> for
documentation.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>When using the Qualcomm® Neural Processing SDK API:</p>
<blockquote>
<div><ul class="simple">
<li><p>Any output layers specified when snpe-dlc-quantize was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputLayers()</span>
function.</p></li>
<li><p>Any output tensors specified when snpe-dlc-quantize was
called, need to be specified using the
<span class="xref std std-ref">Snpe_SNPEBuilder_SetOutputTensors()</span>
function.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="snpe-udo-package-generator">
<h1>snpe-udo-package-generator<a class="headerlink" href="#snpe-udo-package-generator" title="Permalink to this headline">¶</a></h1>
<div class="highlight-fragment notranslate"><div class="highlight"><pre><span></span>DESCRIPTION:
------------
This tool generates a UDO (User Defined Operation) package using a
user provided config file.

USAGE:
------------
snpe-udo-package-generator [-h] --config_path CONFIG_PATH [--debug]
                                    [--output_path OUTPUT_PATH] [-f]
OPTIONAL ARGUMENTS:
-------------------
    -h, --help            show this help message and exit
    --debug               Returns debugging information from generating the package
    --output_path OUTPUT_PATH, -o OUTPUT_PATH
                        Path where the package should be saved
    -f, --force-generation
                        This option will delete the existing package
                        Note appropriate file permissions must be set to use
                        this option.

REQUIRED_ARGUMENTS:
-------------------
    --config_path CONFIG_PATH, -p CONFIG_PATH
                        The path to a config file that defines a UDO.
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020-2023, Qualcomm Technologies, Inc..

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>